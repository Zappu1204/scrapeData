{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lxml in c:\\users\\giap\\anaconda3\\envs\\medicines\\lib\\site-packages (5.2.1)\n",
      "Requirement already satisfied: bs4 in c:\\users\\giap\\anaconda3\\envs\\medicines\\lib\\site-packages (0.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\giap\\anaconda3\\envs\\medicines\\lib\\site-packages (2.31.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\giap\\anaconda3\\envs\\medicines\\lib\\site-packages (from bs4) (4.12.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\giap\\anaconda3\\envs\\medicines\\lib\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\giap\\anaconda3\\envs\\medicines\\lib\\site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\giap\\anaconda3\\envs\\medicines\\lib\\site-packages (from requests) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\giap\\anaconda3\\envs\\medicines\\lib\\site-packages (from requests) (2024.2.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\giap\\anaconda3\\envs\\medicines\\lib\\site-packages (from beautifulsoup4->bs4) (2.5)\n"
     ]
    }
   ],
   "source": [
    "! pip install lxml bs4 requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import lxml\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lấy thông tin từ portal.ptit.edu.vn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "global notificationDataFile\n",
    "global preprocessedData\n",
    "global contentData\n",
    "\n",
    "# Lấy đường dẫn thư mục hiện tại của file script\n",
    "current_dir = os.getcwd()\n",
    "current_dir = os.path.dirname(current_dir)\n",
    "# Tạo thư mục result nếu chưa tồn tại\n",
    "result_path = os.path.join(current_dir, 'result')\n",
    "if not os.path.exists(result_path):\n",
    "    os.makedirs(result_path)\n",
    "\n",
    "# notificationDataFile = os.path.join(result_path, 'noti_from_hv.json')\n",
    "# preprocessedData = os.path.join(result_path, 'data_preproc_from_hv.json') # data preprocessing file\n",
    "# contentData = os.path.join(result_path, 'contens_from_hv.json')\n",
    "\n",
    "# notificationDataFile = os.path.join(result_path, 'noti_from_pgv.json')\n",
    "# preprocessedData = os.path.join(result_path, 'data_preproc_from_pgv.json') # data preprocessing file\n",
    "# contentData = os.path.join(result_path, 'contens_from_pgv.json')\n",
    "\n",
    "# notificationDataFile = os.path.join(result_path, 'news.json')\n",
    "# preprocessedData = os.path.join(result_path, 'data_preproc_news.json') # data preprocessing file\n",
    "# contentData = os.path.join(result_path, 'contens_news.json')\n",
    "\n",
    "notificationDataFile = os.path.join(result_path, 'notifications.json')\n",
    "preprocessedData = os.path.join(result_path, 'data_preproc_notifications.json') # data preprocessing file\n",
    "contentData = os.path.join(result_path, 'contens_notifications.json')\n",
    "\n",
    "# 40 with news and 5 with notifications (info from ~2023)\n",
    "maxPage = 5 # Max is 14~15 with notifications from ptit and 15 with notifications from GV (info from ~2023)\n",
    "\n",
    "# base_url = 'https://portal.ptit.edu.vn/giaovu/category/thong-bao/thong-bao-tu-ho%cc%a3c-vie%cc%a3n/'\n",
    "# base_url = 'https://portal.ptit.edu.vn/giaovu/category/thong-bao/thong-bao-tu-phonggv/'\n",
    "# base_url = 'https://portal.ptit.edu.vn/category/tin-tuc/'\n",
    "base_url = 'https://portal.ptit.edu.vn/category/thong-bao/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "portalData = []\n",
    "\n",
    "def scrapePortalData(url):\n",
    "    global portalData\n",
    "    response = requests.get(url)\n",
    "\n",
    "    soup = BeautifulSoup(response.text, from_encoding='utf-8', features='lxml')\n",
    "    divs = soup.find('div', class_='posts_group lm_wrapper classic col-3')\n",
    "    info_s = divs.find_all('div')\n",
    "    \n",
    "    for item in info_s:\n",
    "        info_item = {}\n",
    "        # Tìm tiêu đề từ class \"entry-title\"\n",
    "        title_tag = item.find('h2', class_='entry-title')\n",
    "        info_item['Title'] = title_tag.get_text().strip() if title_tag else ''\n",
    "        info_item['Link'] = title_tag.find('a')['href'] if title_tag and title_tag.find('a') else ''\n",
    "\n",
    "        # Tìm nội dung từ class \"post-excerpt\"\n",
    "        excerpt_tag = item.find('div', class_='post-excerpt')\n",
    "        info_item['Content'] = excerpt_tag.get_text().strip() if excerpt_tag else ''\n",
    "\n",
    "        # Tìm ngày đăng từ class \"date_label\"\n",
    "        date_tag = item.find('div', class_='date_label')\n",
    "        info_item['Date'] = date_tag.get_text().strip() if date_tag else ''\n",
    "\n",
    "        portalData.append(info_item)\n",
    "\n",
    "def saveDataToJson():\n",
    "    with open(notificationDataFile, 'w', encoding='utf-8-sig') as file:\n",
    "        json.dump(portalData, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "for i in range(1, maxPage + 1):\n",
    "    scrapePortalData(base_url + 'page/' + str(i))\n",
    "\n",
    "saveDataToJson()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xử lý dữ liệu JSON\n",
    "def non_empty_fields(item):\n",
    "    # Đếm số lượng trường không rỗng trong một item\n",
    "    non_empty_count = sum(1 for value in item.values() if value)\n",
    "    return non_empty_count, item\n",
    "\n",
    "def convert_date(date_str):\n",
    "    # Chuyển đổi ngày từ chuỗi sang đối tượng datetime, hoặc trả về None nếu không hợp lệ\n",
    "    try:\n",
    "        return datetime.strptime(date_str, '%d/%m/%Y')\n",
    "    except (ValueError, TypeError):\n",
    "        return None\n",
    "\n",
    "def process_json_data(data):\n",
    "    # Tạo list mới chỉ chứa items không hoàn toàn rỗng và thêm 'info_fullness' và 'Date' chuyển đổi\n",
    "    non_empty_items = [item for item in data if any(item.values())]\n",
    "    for item in non_empty_items:\n",
    "        item['info_fullness'] = non_empty_fields(item)[0]\n",
    "        item['Date'] = convert_date(item['Date']) if item['Date'] else None\n",
    "\n",
    "    # Sắp xếp list dựa trên 'info_fullness', sau đó theo 'Date' nếu có\n",
    "    not_empty_sorted = sorted(non_empty_items, key=lambda x: (-x['info_fullness'], x['Date'] if x['Date'] else datetime.min))\n",
    "\n",
    "    # Loại bỏ những items trùng lặp, giữ lại items với 'info_fullness' cao nhất\n",
    "    seen = set()\n",
    "    unique_data = []\n",
    "    for item in not_empty_sorted:\n",
    "        # Tạo tuple của 'Title', 'Link' để kiểm tra trùng lặp\n",
    "        title_link = (item.get('Title'), item.get('Link'))\n",
    "        if title_link not in seen:\n",
    "            seen.add(title_link)\n",
    "            unique_data.append(item)\n",
    "\n",
    "    # Cuối cùng, sắp xếp kết quả dựa theo 'Date' giảm dần\n",
    "    final_data = sorted(unique_data, key=lambda x: x['Date'] if x['Date'] else datetime.min, reverse=True)\n",
    "    \n",
    "    # Loại bỏ field 'info_fullness' và item nào không có 'Date' hợp lệ\n",
    "    final_cleaned_data = [item for item in final_data if item['Date']]\n",
    "    # Chuyển đổi 'Date' thành chuỗi\n",
    "    for item in final_cleaned_data:\n",
    "        if item['Date']:\n",
    "            item['Date'] = item['Date'].strftime('%d/%m/%Y')\n",
    "\n",
    "    for item in final_cleaned_data:\n",
    "        del item['info_fullness']\n",
    "    \n",
    "    return final_cleaned_data\n",
    "\n",
    "# Chuẩn bị dữ liệu từ file JSON\n",
    "with open(notificationDataFile, 'r', encoding='utf-8-sig') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Xử lý dữ liệu\n",
    "cleaned_data = process_json_data(data)\n",
    "\n",
    "# Ghi dữ liệu đã được làm sạch vào file JSON mới\n",
    "with open(preprocessedData, 'w', encoding='utf-8-sig') as file:\n",
    "    json.dump(cleaned_data, file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_contents_from_page(url):\n",
    "    response = requests.get(url)\n",
    "\n",
    "    soup = BeautifulSoup(response.text, from_encoding='utf-8', features='lxml')\n",
    "\n",
    "    div_contents = soup.find('div', class_='the_content_wrapper')\n",
    "    contents = div_contents.find_all(['p', 'li'], recursive=True)\n",
    "    \n",
    "    content_collection = ''\n",
    "    if contents:\n",
    "        for content in contents:\n",
    "            content_collection = ' '.join([content_collection, content.get_text().strip()])\n",
    "\n",
    "    return content_collection\n",
    "\n",
    "# Đọc dữ liệu từ file JSON\n",
    "with open(preprocessedData, 'r', encoding='utf-8-sig') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Lấy nội dung từ mỗi liên kết và thêm vào mỗi item\n",
    "for item in data:\n",
    "    contents_url = item['Link']\n",
    "    new_content = get_contents_from_page(contents_url)\n",
    "    item['Content'] = new_content\n",
    "\n",
    "# Ghi dữ liệu làm sạch kèm nội dung vào file JSON mới\n",
    "with open(contentData, 'w', encoding='utf-8-sig') as file:\n",
    "    json.dump(data, file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xóa file JSON tạm thời\n",
    "os.remove(notificationDataFile)\n",
    "os.remove(preprocessedData)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "getdata",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
